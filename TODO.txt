--------------------------------------------------------------------------------
SAMPLE DATA: "Chevrolet Chevelle Malibu;18.0;8;307.0;130.0;3504.;12.0;70;US"
--------------------------------------------------------------------------------
User Provides:
a files nested dictionary:
        {filename_1: {outputfile_1: lambda filter
                     outputfile_2: lambda filter
                     outputfile_3: lambda filter}

        filename_1: {outputfile_1: lambda filter
                     outputfile_2: lambda filter
                     outputfile_3: lambda filter}}

--------------------------------------------------------------------------------
# TODO: CHECKLIST:
--------------------------------------------------------------------------------
# TODO: functions take in targets as arguments
# TODO: concatenate file name
# TODO: join os path
# TODO: pytest capturing of the stdout/stderr output
# TODO: pytest temp directory and testing csv
# TODO: Pair together output_files, predicates, and class_names
--------------------------------------------------------------------------------

USER_INPUT:
FORMAT:
zip up output files and filter predicates, broadcaster sends them to
filter_data () and save_data()

you need a date_parser and a valid_date_detector - DONE

date_detector returns a lambda function with the date_format - DONE

go first to date_detector --> mark list as date_func or None - DONE
                          \_> send d(f) and - DONE
maybe we need a fixture for the input of date/data parsers - DONE
    fixture will create file, read line, return headers per row? - DONE

detector detects a date_column and parses it and returns date function - DONE

data_function does what parse_date is currently doing - DONE

zip up inputfile, classname, outputfile, and predicates - DONE

zip up file/predicate tuples into one big tuple,
 pass that to pipeline_coro() - DONE
    # files and named tuples can be zipped - DONE
    # predicates and output files can be zipped - DONE

row_parse_key_gen--> returns the parsing key for parse_data_row - DONE

    file_handler --> opens all the files and closes them: - DONE
    (the only context manager in the program) - DONE
        for file in files_dictionary:
        with file open:
            -read the row --> send the row for header extraction - DONE
                         \--> send the row for data type key generation - DONE


        -extract headers --> create named tuple named tuple send next(line)
        -infer data type--> sample the line and return a string - DONE
        -sample data row(take in parse key) - DONE
            sample data row call next  #now at data lines - DONE
    data_reader --> cast - DONE

    headers extract gets sent user defined headers,
    and sends those to filters - WIP

    filter_predicates --> recieves tracking headers yield 1

    filters -->
                recieves named tuple yield 2
                if all namedtuple values exist in named tuple from tracking
                fileds send to save data else consume it

    broadcaster -->
    broadcaster = broadcast(filters) # filters = targets

    def pred_ford_green(data_row):
        return (data_row[idx_make].lower() == 'ford'
                and data_row[idx_color].lower() == 'green')

    filter_ford_green = filter_data(pred_ford_green, out_ford_green)

    filter_older = filter_data(lambda d: d[idx_year] <= 2010, out_older)
        filter_pink_cars = filter_data
        (lambda d: d[idx_color].lower() == 'pink', out_pink_cars)

        filter_ford_green = filter_data(pred_ford_green, out_ford_green)

        filter_older = filter_data(lambda d: d[idx_year] <= 2010, out_older)

        filters = (filter_pink_cars, filter_ford_green, filter_older)
        broadcaster = broadcast(filters)

# track fields --> Deepak's filter tracker
def filterpredicate(tuple: track_fileds):
    for track_field in track_fields:
        if all(namedtuple.track_field for trackfield in trackfields):
            return named.tuple


    while True:
        data_row = yield
        broadcaster.send(data_row)


-------------------------------------------------------------------------------
@coroutine
def pipeline_coro():
    out_pink_cars = save_data('pink_cars.csv', headers)
    out_ford_green = save_data('ford_green.csv', headers)
    out_older = save_data('older.csv', headers)

    filter_pink_cars = filter_data(lambda d: d[idx_color].lower() == 'pink',
                                  out_pink_cars)

    def pred_ford_green(data_row):
        return (data_row[idx_make].lower() == 'ford'
               and data_row[idx_color].lower() == 'green')
    filter_ford_green = filter_data(pred_ford_green, out_ford_green)
    filter_older = filter_data(lambda d: d[idx_year] <= 2010, out_older)

    filters = (filter_pink_cars, filter_ford_green, filter_older)

    broadcaster = broadcast(filters)

    while True:
        data_row = yield
        broadcaster.send(data_row)


-------------------------------------------------------------------------------
# MISC NOTES
-------------------------------------------------------------------------------
# def get_dialect(file_obj):
# def get_dialect(file_obj):
#     sample = file_obj.read(2000)
#     dialect = csv.Sniffer().sniff(sample)
#     file_obj.seek(0)
#     return dialect


# Vehicle_Info
# Employment_Info
# Ticket_Info
# Personal_Info
# Update_Status


# ------------------------------------------------------------------------------
Execution Order:
# ------------------------------------------------------------------------------

REFACTOR:
    at every stage, each part of the pipe operates on all rows of all files
    at once. Header all rows, yield all rows, data flows together through
    the piple thorugh each stage. - DONE

@coroutine
pipeline_coro
    with file_readers(data_package) as readers:

        (with file_readers as readers:
        for each reader in readers -->
        cycle through the list of readers and send out 5 rows at once
        exception stop iteration, then skip that file
        keep going until every file is exhausted.) - DONE

    -> every part of the pipe awaits 5 row package, 1 from each file
    (number of files/rows)
    -> then after getting eveyr row proceeds to the enxt stage

    -> next(reader) = headers - DONE
    extract queues up header rows for len of (readers) - DONE
    - send date_tuple to date_key_gen y1 - DONE
    - headerextract.send(next(reader)) then to headers - DONE
    - send class names to gen_field_names - DONE
    - send first rows to header_extract - DONE
    - which then sends the lower case header to gen_field_names - DONE

    DELIMITED ROW:
    second row..key_generators (data and row)
        send to:
            date_key_gen -->delimited row y2
            row_key_gen
                which sends partial to date_key_gen y3 --> while ON
    date_key_gen sends final parse_key to row_parser y2
    finally: second row to data_parser y3 --> while ON

    data_parser sends parsed data to broadcaster y1

    broadcaster sends to filters y1 y1

    filters send to save data y1

    save data sends to files y1
    for filter, outfile

    each files filename, classname, filters, and output files should be packages
     togehter if you want to process per file

    if you want to process all files at once then each coroutine should be
    caught in a while loop in total, where each filter recieves
    for each file, read a row
       for each row,
           for each predicate
               send to savefile directory name
               send to savefile output name
                   send to savefile data from filter


    to process each file one row at a time:
        data packet:
                input_file
                dir name
                class_name
                output_file
                filters
# ------------------------------------------------------------------------------
with open('a', 'w') as a, open('b', 'w') as b:
    do_something()
# ------------------------------------------------------------------------------

with ExitStack() as stack:
    files = [stack.enter_context(open(fname)) for fname in filenames]
    # Do something with "files"

    input_pack = {inputfile1: {filters:outputfile}
                  inputfile2: {filters:outputfile}
                  inputfile3: {filters:outputfile}
                  inputfile4: {filters:outputfile}}

# # ----------------------------------------------------------------------------
TEMP:
# # ----------------------------------------------------------------------------
            # send next row to gen_row_parse_key
            # named_tuple_gen sends to data_caster for parsing
            # parser needs named tupel and data type key
            # send the first row of the file to the header function
            # send first row to gen_field_names
            # header function sends to gen_field_name
            # gen_parse key sends key to caster
            # header_extract.send(next(f))  # --> send row for header extract
            # row_parse_key_gen.send(next(f))
            # date_parse gen

        # for data_package in data_packages:
        #    for inputfile, classname, outputfile, predicate in datapackage:
        #    do stuff:
        # instantiate functions parameters..
        # can you instantiate without symbol binding?
        # # instance save data writers:
        # out_pink_cars = save_data('pink_cars.csv', header_extract(fcars))
        # out_ford_green = save_data('ford_green.csv', header_extract(fcars))
        # out_older = save_data('older.csv', header_extract(fcars))

        # filter instances with predicates
        # filter_pink_cars = filter_data(lambda d: d[idx_color].lower() ==
        # 'pink',
        #                                out_pink_cars)

        # predicates can be defined as filters..
        # def pred_ford_green(data_row):
        #     return (data_row[idx_make].lower() == 'ford'
        #             and data_row[idx_color].lower() == 'green')
        # filter_ford_green = filter_data(pred_ford_green, out_ford_green)
        # filter_older = filter_data(lambda d: d[idx_year] <= 2010, out_older)
        # filters = (filter_pink_cars, filter_ford_green, filter_older)
        # your brodcaster must send data from row
        # broadcaster = broadcast(filters)
        # while True:
        #     data_row = yield
        #     broadcaster.send(data_row)
# # ----------------------------------------------------------------------------

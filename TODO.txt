-------------------------------------------------------------------------------
SAMPLE DATA: "Chevrolet Chevelle Malibu;18.0;8;307.0;130.0;3504.;12.0;70;US"
-------------------------------------------------------------------------------
User Provides:
a files nested dictionary:
        {filename_1: {outputfile_1: lambda filter
                     outputfile_2: lambda filter
                     outputfile_3: lambda filter}

        filename_1: {outputfile_1: lambda filter
                     outputfile_2: lambda filter
                     outputfile_3: lambda filter}}
--------------------------------------------------------------------------------
# TODO: CHECKLIST:
--------------------------------------------------------------------------------
# TODO: functions take in targets as arguments

# TODO: concatenate file name
# TODO: join os path

# TODO: pytest capturing of the stdout/stderr output
# TODO: pytest temp directory and testing csv
# TODO: Pair together output_files, predicates, and class_names
--------------------------------------------------------------------------------
USER_INPUT:
FORMAT:
zip up output files and filter predicates, broadcaster sends them to
filter_data () and save_data()

you need a date_parser and a valid_date_detector

date_detector returns a lambda function with the date_format

detector detects a date_column and parses it and returns date function

data_function does what parse_date is currently doing

zip up inputfile, classname, outputfile, and predicates

zip up file/predicate tuples into one big tuple, pass that to pipeline_coro()
    # files and named tuples can be zipped
    # predicates and output files can be zipped

row_parse_key_gen--> returns the parsing key for parse_data_row

    file_handler --> opens all the files and closes them:
    (the only context manager in the program)
        for file in files_dictionary:
        with file open:
            -read the row --> send the row for header extraction
                         \--> send the row for data type key generation


        -extract headers --> create named tuple named tuple send next(line) to
        infer

        -infer data type--> sample the line and return a string

        -sample data row(take in parse key)
            sample data row call next  #now at data lines


    data_reader --> cast

    headers extract gets sent user defined headers, and sends those to filters

    filter_predicates --> recieves tracking headers yield 1

    filters -->
                recieves named tuple yield 2
                if all namedtuple values exist in named tuple from tracking
                fileds send to save data else consume it

    broadcaster -->
    broadcaster = broadcast(filters) # filters = targets
    
    def pred_ford_green(data_row):
        return (data_row[idx_make].lower() == 'ford'
                and data_row[idx_color].lower() == 'green')

    filter_ford_green = filter_data(pred_ford_green, out_ford_green)

    filter_older = filter_data(lambda d: d[idx_year] <= 2010, out_older)
        filter_pink_cars = filter_data
        (lambda d: d[idx_color].lower() == 'pink', out_pink_cars)

        filter_ford_green = filter_data(pred_ford_green, out_ford_green)

        filter_older = filter_data(lambda d: d[idx_year] <= 2010, out_older)

        filters = (filter_pink_cars, filter_ford_green, filter_older)
        broadcaster = broadcast(filters)

# track fields --> Deepak's filter tracker
def filterpredicate(tuple: track_fileds):
    for track_field in track_fields:
        if all(namedtuple.track_field for trackfield in trackfields):
            return named.tuple


    while True:
        data_row = yield
        broadcaster.send(data_row)


-------------------------------------------------------------------------------
@coroutine
def pipeline_coro():
    out_pink_cars = save_data('pink_cars.csv', headers)
    out_ford_green = save_data('ford_green.csv', headers)
    out_older = save_data('older.csv', headers)

    filter_pink_cars = filter_data(lambda d: d[idx_color].lower() == 'pink',
                                  out_pink_cars)

    def pred_ford_green(data_row):
        return (data_row[idx_make].lower() == 'ford'
               and data_row[idx_color].lower() == 'green')
    filter_ford_green = filter_data(pred_ford_green, out_ford_green)
    filter_older = filter_data(lambda d: d[idx_year] <= 2010, out_older)

    filters = (filter_pink_cars, filter_ford_green, filter_older)

    broadcaster = broadcast(filters)

    while True:
        data_row = yield
        broadcaster.send(data_row)


-------------------------------------------------------------------------------
# MISC NOTES
-------------------------------------------------------------------------------
# def get_dialect(file_obj):
# def get_dialect(file_obj):
#     sample = file_obj.read(2000)
#     dialect = csv.Sniffer().sniff(sample)
#     file_obj.seek(0)
#     return dialect


# Vehicle_Info
# Employment_Info
# Ticket_Info
# Personal_Info
# Update_Status


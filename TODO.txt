--------------------------------------------------------------------------------
SAMPLE DATA: "Chevrolet Chevelle Malibu;18.0;8;307.0;130.0;3504.;12.0;70;US"
--------------------------------------------------------------------------------
User Provides:
a files nested dictionary:
        {filename_1: {outputfile_1: lambda filter
                     outputfile_2: lambda filter
                     outputfile_3: lambda filter}

        filename_1: {outputfile_1: lambda filter
                     outputfile_2: lambda filter
                     outputfile_3: lambda filter}}
--------------------------------------------------------------------------------
# TODO: CHECKLIST:
--------------------------------------------------------------------------------
# TODO: functions take in targets as arguments

# TODO: concatenate file name
# TODO: join os path

# TODO: pytest capturing of the stdout/stderr output
# TODO: pytest temp directory and testing csv
# TODO: Pair together output_files, predicates, and class_names
--------------------------------------------------------------------------------
USER_INPUT:
FORMAT:
zip up output files and filter predicates, broadcaster sends them to
filter_data () and save_data()

you need a date_parser and a valid_date_detector - DONE

date_detector returns a lambda function with the date_format - DONE

go first to date_detector --> mark list as date_func or None
                          \_> send d(f) and
maybe we need a fixture for the input of date/data parsers
    fixture will create file, read line, return headers per row?

detector detects a date_column and parses it and returns date function - WIP

data_function does what parse_date is currently doing

zip up inputfile, classname, outputfile, and predicates

zip up file/predicate tuples into one big tuple, pass that to pipeline_coro()
    # files and named tuples can be zipped
    # predicates and output files can be zipped

row_parse_key_gen--> returns the parsing key for parse_data_row

    file_handler --> opens all the files and closes them:
    (the only context manager in the program)
        for file in files_dictionary:
        with file open:
            -read the row --> send the row for header extraction
                         \--> send the row for data type key generation


        -extract headers --> create named tuple named tuple send next(line) to

        dd=
        infer

        -infer data type--> sample the line and return a string

        -sample data row(take in parse key)
            sample data row call next  #now at data lines


    data_reader --> cast

    headers extract gets sent user defined headers, and sends those to filters

    filter_predicates --> recieves tracking headers yield 1

    filters -->
                recieves named tuple yield 2
                if all namedtuple values exist in named tuple from tracking
                fileds send to save data else consume it

    broadcaster -->
    broadcaster = broadcast(filters) # filters = targets

    def pred_ford_green(data_row):
        return (data_row[idx_make].lower() == 'ford'
                and data_row[idx_color].lower() == 'green')

    filter_ford_green = filter_data(pred_ford_green, out_ford_green)

    filter_older = filter_data(lambda d: d[idx_year] <= 2010, out_older)
        filter_pink_cars = filter_data
        (lambda d: d[idx_color].lower() == 'pink', out_pink_cars)

        filter_ford_green = filter_data(pred_ford_green, out_ford_green)

        filter_older = filter_data(lambda d: d[idx_year] <= 2010, out_older)

        filters = (filter_pink_cars, filter_ford_green, filter_older)
        broadcaster = broadcast(filters)

# track fields --> Deepak's filter tracker
def filterpredicate(tuple: track_fileds):
    for track_field in track_fields:
        if all(namedtuple.track_field for trackfield in trackfields):
            return named.tuple


    while True:
        data_row = yield
        broadcaster.send(data_row)


-------------------------------------------------------------------------------
@coroutine
def pipeline_coro():
    out_pink_cars = save_data('pink_cars.csv', headers)
    out_ford_green = save_data('ford_green.csv', headers)
    out_older = save_data('older.csv', headers)

    filter_pink_cars = filter_data(lambda d: d[idx_color].lower() == 'pink',
                                  out_pink_cars)

    def pred_ford_green(data_row):
        return (data_row[idx_make].lower() == 'ford'
               and data_row[idx_color].lower() == 'green')
    filter_ford_green = filter_data(pred_ford_green, out_ford_green)
    filter_older = filter_data(lambda d: d[idx_year] <= 2010, out_older)

    filters = (filter_pink_cars, filter_ford_green, filter_older)

    broadcaster = broadcast(filters)

    while True:
        data_row = yield
        broadcaster.send(data_row)


-------------------------------------------------------------------------------
# MISC NOTES
-------------------------------------------------------------------------------
# def get_dialect(file_obj):
# def get_dialect(file_obj):
#     sample = file_obj.read(2000)
#     dialect = csv.Sniffer().sniff(sample)
#     file_obj.seek(0)
#     return dialect


# Vehicle_Info
# Employment_Info
# Ticket_Info
# Personal_Info
# Update_Status


# ------------------------------------------------------------------------------
Execution Order:
# ------------------------------------------------------------------------------

decide:
    at every stage, each part of the pipe operates on all rows at once
    header all rows, yield all rows, data flows together through the piple
    thorugh each stage

@coroutine
pipeline_coro
    @sub-contextmanager-ExitStack:
        with file_readers as readers:
        for each reader in readers -->
        cycle through the list of readers
        exception stop iteration, then skip that file
        keep going until every file is exhausted.

    -> every part of the pipe awaits thes first #of incoming data packets
    (number of files/rows)
    -> then after getting eveyr row proceeds to the enxt stage
    -> OR filehandler can cyclically reader every row and send it the
    through the pipe

    -> send len(readers) of each file
    -> next(reader) = headers
    extract queues up header rows for len of (readers)
    - send date_tuple to date_key_gen y1
    - headerextract.send(next(reader)) then to headers
    - send class names to gen_field_names
    - send first row to header_extract
    - which then sends the lower case header to gen_field_names


    DELIMITED ROW:
    second row..key_generators (data and row)
        send to:
            date_key_gen -->delimited row y2
            row_key_gen
                which sends partial to date_key_gen y3 --> while ON

    date_key_gen sends final parse_key to row_parser y2
    finally: second row to data_parser y3 --> while ON

    data_parser sends parsed data to broadcaster y1

    broadcaster sends to filters y1 y1

    filters send to save data y1

    save data sends to files y1
    for filter, outfile

    each files filename, classname, filters, and output files should be packages
     togehter if you want to process per file

    if you want to process all files at once then each coroutine should be
    caught in a while loop in total, where each filter recieves
    for each file, read a row
       for each row,
           for each predicate
               send to savefile directory name
               send to savefile output name
                   send to savefile data from filter


    to process each file one row at a time:
        data packet:
                input_file
                dir name
                class_name
                output_file
                filters
# ------------------------------------------------------------------------------
with open('a', 'w') as a, open('b', 'w') as b:
    do_something()
# ------------------------------------------------------------------------------

with ExitStack() as stack:
    files = [stack.enter_context(open(fname)) for fname in filenames]
    # Do something with "files"

    input_pack = {inputfile1: {filters:outputfile}
                  inputfile2: {filters:outputfile}
                  inputfile3: {filters:outputfile}
                  inputfile4: {filters:outputfile}}

